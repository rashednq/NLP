{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic 100k Reviews (Part 2): Classifier\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is the second part of the Arabic 100k Reviews classification pipeline. Building on the preprocessing work from Part 1, we now focus on building and evaluating a text classification model to predict sentiment (Positive or Negative) from Arabic reviews. This notebook covers vectorization, model training, evaluation, and interpretation of results, demonstrating a complete end-to-end NLP classification pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Refer to the README.md for lab setup instructions\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Build a text classification model for Arabic sentiment analysis\n",
    "- Apply vectorization techniques (TF-IDF) to convert preprocessed text into numerical features\n",
    "- Train and evaluate a machine learning classifier (Logistic Regression)\n",
    "- Understand model evaluation metrics (accuracy, precision, recall, F1-score, confusion matrix)\n",
    "- Interpret classification results and analyze model performance\n",
    "- Recognize the importance of proper train/test splitting and data preprocessing\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. **Setup and Imports** - Installing dependencies and importing libraries\n",
    "2. **Data Loading** - Loading preprocessed data from Part 1\n",
    "3. **Text Analytics (EDA)** - Analyzing the preprocessed dataset\n",
    "4. **Vectorization** - Converting text to numerical features using TF-IDF\n",
    "5. **Model Training** - Training a Logistic Regression classifier\n",
    "6. **Model Evaluation** - Assessing performance with various metrics\n",
    "7. **Results Interpretation** - Understanding model predictions and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = pd.read_csv('ar_reviews_100k_cleaned.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = df_normal.loc[45:50].index\n",
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal.loc[sample_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this, let's count the number of unique tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [item for sublist in df_normal['stemmed_tokens'] for item in sublist]\n",
    "token_counter = Counter(all_tokens)\n",
    "token_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Student** Exercise\n",
    "\n",
    "WHAT HAPPENED? WHY ARE WE GETTING THIS OUTPUT?:\n",
    "\n",
    "```\n",
    "[(\"'\", 428696),\n",
    " (',', 205347),\n",
    " (' ', 205347),\n",
    " ('ر', 53548),\n",
    " ('ل', 48756),\n",
    " ('ا', 43960),\n",
    " ('م', 37622),\n",
    " ('ن', 34172),\n",
    " ('ي', 33837),\n",
    " ('د', 33724)]\n",
    " ```\n",
    "\n",
    "> Your task is to fix the issue above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# 1. Convert the string representation back to a real list\n",
    "# We apply this only if the type is currently a string\n",
    "df_normal['stemmed_tokens'] = df_normal['stemmed_tokens'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# 2. Now run your original code\n",
    "all_tokens = [item for sublist in df_normal['stemmed_tokens'] for item in sublist]\n",
    "\n",
    "# Test it\n",
    "print(all_tokens[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [item for sublist in df_normal['stemmed_tokens'] for item in sublist]\n",
    "token_counter = Counter(all_tokens)\n",
    "token_counter.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique words do we have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ratio = len(token_counter) / len(all_tokens)\n",
    "print(f'Unique ratio: {unique_ratio:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out which words are associated with positive and negative labels, and which aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split counts based on labels\n",
    "counter_positive = Counter([item for sublist in df_normal.loc[df_normal['label'] == 'Positive', 'stemmed_tokens'] for item in sublist])\n",
    "counter_negative = Counter([item for sublist in df_normal.loc[df_normal['label'] == 'Negative', 'stemmed_tokens'] for item in sublist])\n",
    "counter_mixed = Counter([item for sublist in df_normal.loc[df_normal['label'] == 'Mixed', 'stemmed_tokens'] for item in sublist])\n",
    "\n",
    "# Purify each label\n",
    "## Positive\n",
    "pure_positive = counter_positive.copy()\n",
    "pure_positive.subtract(counter_negative)\n",
    "pure_positive.subtract(counter_mixed)\n",
    "\n",
    "## Negative\n",
    "pure_negative = counter_negative.copy()\n",
    "pure_negative.subtract(counter_positive)\n",
    "pure_negative.subtract(counter_mixed)\n",
    "\n",
    "## Neutral\n",
    "pure_mixed = counter_mixed.copy()\n",
    "pure_mixed.subtract(counter_positive)\n",
    "pure_mixed.subtract(counter_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    pure_positive.most_common(20),\n",
    "    columns=['token', 'count']\n",
    ").style.background_gradient(cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    pure_negative.most_common(20),\n",
    "    columns=['token', 'count']\n",
    ").style.background_gradient(cmap='Reds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the neutral words that are not in the positive or negative\n",
    "pd.DataFrame(\n",
    "    pure_mixed.most_common(20),\n",
    "    columns=['token', 'count']\n",
    ").style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal.loc[sample_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Text Classification\n",
    "\n",
    "Now that we have cleaned and preprocessed our text data, we can build a **text classifier** that predicts the sentiment (Positive, Negative, or Mixed) of Arabic reviews.\n",
    "\n",
    "**What is Text Classification?**\n",
    "\n",
    "Text classification is a supervised machine learning task where we:\n",
    "1. **Extract features** from text (convert text to numbers)\n",
    "2. **Train a model** to learn patterns between features and labels\n",
    "3. **Predict** the class of new, unseen text\n",
    "\n",
    "**Why use word counts?**\n",
    "\n",
    "After cleaning and preprocessing, we have a list of tokens (words) for each review. One simple but effective approach is to:\n",
    "- Count how many times each word appears in each document\n",
    "- Use these counts as features for our classifier\n",
    "- This is called **Bag of Words (BoW)** representation\n",
    "\n",
    "**The Bag of Words Model:**\n",
    "\n",
    "The Bag of Words model represents text as a vector of word counts, ignoring word order and grammar. For example:\n",
    "\n",
    "- Review 1: \"ممتاز رائع\" → `{\"ممتاز\": 1, \"رائع\": 1}`\n",
    "- Review 2: \"ممتاز ممتاز سيء\" → `{\"ممتاز\": 2, \"رائع\": 0, \"سيء\": 1}`\n",
    "\n",
    "This creates a feature matrix where:\n",
    "- Each row = one review\n",
    "- Each column = one unique word in the vocabulary\n",
    "- Each cell = count of that word in that review\n",
    "\n",
    "**Why this works:**\n",
    "\n",
    "From our EDA, we saw that certain words are more associated with positive reviews (e.g., \"ممتاز\", \"رائع\") and others with negative reviews (e.g., \"سيء\", \"ضعيف\"). A classifier can learn these patterns from the word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Text Data for Feature Extraction\n",
    "\n",
    "Our `stemmed_tokens` column contains lists of tokens. To use scikit-learn's `CountVectorizer`, we need to convert these token lists back into text strings (space-separated words).\n",
    "\n",
    "**Why convert back to strings?**\n",
    "\n",
    "- `CountVectorizer` expects text input (strings)\n",
    "- It will handle tokenization internally, but since we've already cleaned and stemmed our text, we want to use our preprocessed tokens\n",
    "- We'll join the tokens with spaces to create clean text strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token lists back to space-separated strings\n",
    "# This allows CountVectorizer to work with our preprocessed tokens\n",
    "df_normal['text_processed'] = df_normal['stemmed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Display a sample to verify\n",
    "print(\"Original text:\")\n",
    "print(df_normal.loc[sample_ids[0], 'text'])\n",
    "print(\"\\nCleaned and processed text:\")\n",
    "print(df_normal.loc[sample_ids[0], 'text_processed'])\n",
    "print(\"\\nTokens used:\")\n",
    "print(df_normal.loc[sample_ids[0], 'stemmed_tokens'][:10])  # Show first 10 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Word Count Features\n",
    "\n",
    "We'll use scikit-learn's `CountVectorizer` to convert our processed text into a matrix of word counts.\n",
    "\n",
    "**Key parameters:**\n",
    "- `max_features`: Limit vocabulary size to the most frequent N words (reduces dimensionality)\n",
    "- `min_df`: Ignore words that appear in fewer than N documents (removes rare words)\n",
    "- `max_df`: Ignore words that appear in more than N% of documents (removes common words that appear everywhere)\n",
    "\n",
    "**Why limit features?**\n",
    "- Reduces memory usage\n",
    "- Speeds up training\n",
    "- Can improve generalization by focusing on meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer (Bag of Words)\n",
    "# We use max_features to limit vocabulary size for efficiency\n",
    "# min_df=2 means words must appear in at least 2 documents\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=5000,  # Use top 5000 most frequent words\n",
    "    min_df=2,           # Word must appear in at least 2 documents\n",
    "    max_df=0.95         # Ignore words that appear in >95% of documents\n",
    ")\n",
    "\n",
    "# Transform text to word count matrix\n",
    "# This creates a sparse matrix where each row is a review and each column is a word\n",
    "X = vectorizer.fit_transform(df_normal['text_processed'])\n",
    "y = df_normal['label'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of reviews: {X.shape[0]}\")\n",
    "print(f\"Number of features (words): {X.shape[1]}\")\n",
    "print(f\"Labels: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Feature Matrix:**\n",
    "\n",
    "The `X` matrix is sparse (mostly zeros) because:\n",
    "- Each review contains only a small subset of all possible words\n",
    "- Most words don't appear in most reviews\n",
    "- This is normal and expected for text data\n",
    "\n",
    "Let's visualize what the feature matrix looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix to dense for visualization (only for small samples!)\n",
    "# Note: For large datasets, keep it sparse to save memory\n",
    "X_sample = X[:5].toarray()\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "df_features = pd.DataFrame(\n",
    "    X_sample,\n",
    "    columns=feature_names,\n",
    "    index=[f\"Review {i+1}\" for i in range(5)]\n",
    ")\n",
    "\n",
    "# Show only columns (words) that appear in these 5 reviews\n",
    "non_zero_cols = df_features.columns[df_features.sum() > 0]\n",
    "print(f\"Showing {len(non_zero_cols)} words that appear in the first 5 reviews:\")\n",
    "display(df_features[non_zero_cols[:20]])  # Show first 20 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split Data into Training and Testing Sets\n",
    "\n",
    "**Why split the data?**\n",
    "\n",
    "We need to:\n",
    "1. **Train** the model on one portion of data\n",
    "2. **Test** the model on unseen data to evaluate its performance\n",
    "3. **Prevent overfitting** - ensure the model generalizes to new data\n",
    "\n",
    "**Important:** We split AFTER preprocessing to avoid data leakage (information from test set influencing training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 80% training, 20% testing\n",
    "# stratify=y ensures same class distribution in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y  # Maintain class balance\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} reviews\")\n",
    "print(f\"Test set: {X_test.shape[0]} reviews\")\n",
    "print(\"\\nTraining label distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a Classifier\n",
    "\n",
    "We'll use a **Logistic Regression** classifier, which is:\n",
    "- **Fast** and efficient for text classification\n",
    "- **Well-suited** for count-based features (like our word counts)\n",
    "- **Simple** to understand and interpret\n",
    "- **Effective** for text classification tasks\n",
    "- **Provides probability estimates** for each class\n",
    "\n",
    "**How Logistic Regression works (simplified):**\n",
    "1. Learns weights for each feature (word) that indicate its importance for each class\n",
    "2. Uses a logistic function to convert weighted sums into probabilities\n",
    "3. Predicts the class with highest probability\n",
    "\n",
    "Logistic Regression is a linear classifier that works well with sparse, high-dimensional text features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the classifier\n",
    "# max_iter=1000 ensures convergence for large datasets\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=SEED)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"Classifier trained successfully!\")\n",
    "print(f\"Number of features learned: {classifier.coef_.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the Classifier\n",
    "\n",
    "Now let's evaluate how well our classifier performs on the test set. We'll use several metrics:\n",
    "\n",
    "- **Accuracy**: Overall percentage of correct predictions\n",
    "- **Precision**: Of all predictions for a class, how many were correct?\n",
    "- **Recall**: Of all actual instances of a class, how many did we find?\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balances both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Classification Report:**\n",
    "\n",
    "- **Precision**: When the model predicts a class, how often is it correct?\n",
    "  - High precision = few false positives\n",
    "  \n",
    "- **Recall**: How many of the actual instances of a class did we catch?\n",
    "  - High recall = few false negatives\n",
    "  \n",
    "- **F1-Score**: Balances precision and recall\n",
    "  - Useful when you need a single metric\n",
    "  \n",
    "- **Support**: Number of actual instances of each class in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix for visualization\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display as DataFrame for better readability\n",
    "cm_df = pd.DataFrame(\n",
    "    cm,\n",
    "    index=[f\"Actual {label}\" for label in classifier.classes_],\n",
    "    columns=[f\"Predicted {label}\" for label in classifier.classes_]\n",
    ")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"(Rows = Actual, Columns = Predicted)\")\n",
    "display(cm_df)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classifier.classes_))\n",
    "plt.xticks(tick_marks, classifier.classes_, rotation=45)\n",
    "plt.yticks(tick_marks, classifier.classes_)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the Confusion Matrix:**\n",
    "\n",
    "- **Diagonal elements** (top-left to bottom-right): Correct predictions\n",
    "- **Off-diagonal elements**: Misclassifications\n",
    "  - Example: If \"Actual Negative\" row has a number in \"Predicted Positive\" column, those are negative reviews incorrectly classified as positive\n",
    "\n",
    "The confusion matrix helps us understand:\n",
    "- Which classes are confused with each other\n",
    "- Whether errors are balanced or biased toward certain classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Analyze Important Features\n",
    "\n",
    "Let's see which words are most important for each class. This helps us understand what the model learned and provides interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get coefficients for each class\n",
    "# Higher values = more indicative of that class\n",
    "coefficients = classifier.coef_\n",
    "\n",
    "# Create DataFrame showing top words for each class\n",
    "top_words_per_class = {}\n",
    "for idx, class_label in enumerate(classifier.classes_):\n",
    "    # Get indices of top words for this class\n",
    "    top_indices = np.argsort(coefficients[idx])[-20:][::-1]  # Top 20 words\n",
    "    top_words = [(feature_names[i], coefficients[idx][i]) for i in top_indices]\n",
    "    top_words_per_class[class_label] = top_words\n",
    "\n",
    "# Display results\n",
    "for class_label, words in top_words_per_class.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Top 20 words for class: {class_label}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    df_top = pd.DataFrame(words, columns=['Word', 'Coefficient']).style.background_gradient(cmap='Greens')\n",
    "    display(df_top.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting Feature Importance:**\n",
    "\n",
    "The coefficients tell us how strongly each word is associated with each class:\n",
    "- **Higher coefficient** = word is more indicative of that class\n",
    "- **Lower (more negative) coefficient** = word is less indicative of that class\n",
    "- Words with high coefficients for \"Positive\" are likely positive sentiment words\n",
    "- Words with high coefficients for \"Negative\" are likely negative sentiment words\n",
    "\n",
    "**Compare with EDA results:** Do these top words match the words we found in our earlier EDA analysis? This validates that the model learned meaningful patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Test on Sample Reviews\n",
    "\n",
    "Let's test our classifier on some example reviews to see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test indices in the original dataframe\n",
    "# train_test_split maintains order, so we need to track which rows went to test set\n",
    "_, test_indices_original = train_test_split(\n",
    "    df_normal.index,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=df_normal['label']\n",
    ")\n",
    "\n",
    "# Select a few random test examples\n",
    "np.random.seed(SEED)\n",
    "# FIX: For sparse matrices, use shape[0] instead of len()\n",
    "sample_test_indices = np.random.choice(X_test.shape[0], size=5, replace=False)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# For a more visual display, let's build a table of predictions and use color to show confidence\n",
    "\n",
    "viz_rows = []\n",
    "for test_idx in sample_test_indices:\n",
    "    original_df_idx = test_indices_original[test_idx]\n",
    "    text_snippet = df_normal.loc[original_df_idx, 'text'][:80] + (\"...\" if len(df_normal.loc[original_df_idx, 'text']) > 80 else \"\")\n",
    "    actual = y_test[test_idx]\n",
    "    prediction = classifier.predict(X_test[test_idx:test_idx+1])[0]\n",
    "    probs = classifier.predict_proba(X_test[test_idx:test_idx+1])[0]\n",
    "    confidence = probs[classifier.classes_.tolist().index(prediction)]\n",
    "    viz_rows.append({\n",
    "        \"Review #\": test_idx,\n",
    "        \"Snippet\": text_snippet,\n",
    "        \"Actual Label\": actual,\n",
    "        \"Predicted Label\": prediction,\n",
    "        \"Confidence\": confidence,\n",
    "        **{f\"P({cls})\": p for cls, p in zip(classifier.classes_, probs)}\n",
    "    })\n",
    "\n",
    "viz_df = pd.DataFrame(viz_rows)\n",
    "# True-match in green, wrong in red\n",
    "def highlight_prediction(row):\n",
    "    color = \"\"\n",
    "    if row['Actual Label'] == row['Predicted Label']:\n",
    "        color = \"background-color: #d4f4dd\"  # light green\n",
    "    else:\n",
    "        color = \"background-color: #f4cccc\"  # light red\n",
    "    return [color]*len(row)\n",
    "# Display styled table, coloring accuracy and using a confidence gradient on \"Confidence\"\n",
    "viz_df_styled = (viz_df.style\n",
    "    .apply(highlight_prediction, axis=1)\n",
    "    .background_gradient(subset=['Confidence'], cmap='Blues')\n",
    "    .format({col: \"{:.2%}\" for col in ['Confidence'] + [f\"P({cls})\" for cls in classifier.classes_]})\n",
    ")\n",
    "display(viz_df_styled)\n",
    "\n",
    "# Also, show barplots of the predicted probability for each review\n",
    "for idx, row in viz_df.iterrows():\n",
    "    plt.figure(figsize=(4,2))\n",
    "    plt.bar(classifier.classes_, [row[f\"P({cls})\"] for cls in classifier.classes_], color=['#d1e0e0' if row['Predicted Label']==cls else '#ffe0b2' for cls in classifier.classes_])\n",
    "    plt.title(f\"Review #{row['Review #']} prediction\\nActual: {row['Actual Label']} | Predicted: {row['Predicted Label']} ({row['Confidence']:.0%})\")\n",
    "    plt.ylabel('Probability')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: What We Learned\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **Simple approaches work**: Word counts (Bag of Words) can be effective for text classification\n",
    "- **Preprocessing matters**: The cleaning, normalization, and stemming we did earlier improved our features\n",
    "- **Interpretability**: We can see which words drive predictions, making the model explainable\n",
    "- **Evaluation is crucial**: Always test on unseen data to measure real-world performance\n",
    "\n",
    "**Next Steps (for future exploration):**\n",
    "\n",
    "- Try **TF-IDF** instead of raw counts (weights words by importance)\n",
    "- Experiment with different classifiers (SVM, Random Forest, etc.)\n",
    "- Use **word embeddings** (like Word2Vec or pre-trained embeddings) for richer features\n",
    "- Fine-tune **transformer models** (like BERT) for state-of-the-art performance\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: This is a foundational approach. Modern NLP often uses more sophisticated methods, but understanding word counts and simple classifiers is essential for building intuition about how text classification works!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
